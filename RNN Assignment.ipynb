{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "490654d2-ff01-446e-8b03-c37c9c40b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e19b4d-2476-4378-814a-cfd55b67d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training sentence and vocabulary\n",
    "sentence = [\"I\", \"am\", \"very\", \"happy\"]\n",
    "vocab = list(set(sentence))\n",
    "vocab_size = len(vocab)\n",
    "hidden_size = 4  # You can change this\n",
    "\n",
    "# Create word-index mappings\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "def one_hot_encode(word):\n",
    "    vec = np.zeros((vocab_size, 1))\n",
    "    vec[word_to_index[word]] = 1\n",
    "    return vec\n",
    "\n",
    "# Initialize RNN weights\n",
    "np.random.seed(42)\n",
    "Wx = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Wy = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "\n",
    "def rnn_forward(inputs, h_prev):\n",
    "    xs, hs, ys = {}, {}, {}\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = one_hot_encode(inputs[t])\n",
    "        hs[t] = np.tanh(np.dot(Wx, xs[t]) + np.dot(Wh, hs[t-1]))\n",
    "        ys[t] = np.dot(Wy, hs[t])\n",
    "        ys[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # Softmax\n",
    "    return xs, hs, ys\n",
    "\n",
    "def compute_loss(ys, target):\n",
    "    target_idx = word_to_index[target]\n",
    "    last_output = ys[len(ys) - 1]\n",
    "    return -np.log(last_output[target_idx, 0])\n",
    "\n",
    "def rnn_backward(xs, hs, ys, target):\n",
    "    global Wx, Wh, Wy\n",
    "    dWx, dWh, dWy = np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(Wy)\n",
    "    dh_next = np.zeros((hidden_size, 1))\n",
    "\n",
    "    dy = np.copy(ys[len(xs) - 1])\n",
    "    dy[word_to_index[target]] -= 1\n",
    "\n",
    "    for t in reversed(range(len(xs))):\n",
    "        dWy += np.dot(dy, hs[t].T)\n",
    "        dh = np.dot(Wy.T, dy) + dh_next\n",
    "        dh_raw = (1 - hs[t] ** 2) * dh\n",
    "        dWx += np.dot(dh_raw, xs[t].T)\n",
    "        dWh += np.dot(dh_raw, hs[t - 1].T)\n",
    "        dh_next = np.dot(Wh.T, dh_raw)\n",
    "        dy = np.zeros_like(dy)  # Only first dy affects loss\n",
    "\n",
    "    # Clip gradients to avoid exploding gradients\n",
    "    for dparam in [dWx, dWh, dWy]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    return dWx, dWh, dWy\n",
    "\n",
    "def train_rnn(inputs, target, epochs=100, learning_rate=0.1):\n",
    "    global Wx, Wh, Wy\n",
    "    h_prev = np.zeros((hidden_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        xs, hs, ys = rnn_forward(inputs, h_prev)\n",
    "        loss = compute_loss(ys, target)\n",
    "        dWx, dWh, dWy = rnn_backward(xs, hs, ys, target)\n",
    "\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        Wy -= learning_rate * dWy\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    return ys\n",
    "\n",
    "def predict_next(inputs):\n",
    "    h_prev = np.zeros((hidden_size, 1))\n",
    "    _, _, ys = rnn_forward(inputs, h_prev)\n",
    "    final_output = ys[len(inputs) - 1]\n",
    "    pred_idx = np.argmax(final_output)\n",
    "    return index_to_word[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd34411-35b5-4b09-bb95-7b963e1cbc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RNN model...\n",
      "Epoch 0, Loss: 1.3863\n",
      "Epoch 10, Loss: 1.3855\n",
      "Epoch 20, Loss: 1.3820\n",
      "Epoch 30, Loss: 1.3641\n",
      "Epoch 40, Loss: 1.2792\n",
      "Epoch 50, Loss: 0.9901\n",
      "Epoch 60, Loss: 0.5421\n",
      "Epoch 70, Loss: 0.2674\n",
      "Epoch 80, Loss: 0.1534\n",
      "Epoch 90, Loss: 0.1017\n",
      "\n",
      "Input Sequence: ['I', 'am', 'very']\n",
      "Predicted Word: happy\n",
      "Actual Word: happy\n"
     ]
    }
   ],
   "source": [
    "# Set up training inputs\n",
    "input_sequence = [\"I\", \"am\", \"very\"]\n",
    "target_word = \"happy\"\n",
    "\n",
    "print(\"Training the RNN model...\")\n",
    "train_rnn(input_sequence, target_word, epochs=100, learning_rate=0.1)\n",
    "\n",
    "# Prediction\n",
    "predicted_word = predict_next(input_sequence)\n",
    "print(f\"\\nInput Sequence: {input_sequence}\")\n",
    "print(f\"Predicted Word: {predicted_word}\")\n",
    "print(f\"Actual Word: {target_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2add8-9d21-4587-9ecf-9973023a264a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
